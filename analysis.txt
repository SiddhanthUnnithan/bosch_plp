Notes
- data represents part measurements
- large number of anonymized features
- can group features based on production and station lines
        - L<prod line>_S<station line>_F<feature id>
- three main feature types: numerical, categorical, date
        - date: timestamp for when measurement was taken
        - n features
        - i_th date column corresponds to i-1_th feature where iE[1, n+1]
        - assuming sequential relation between measurements and dates
        - (e.g. x measurements => x dates => 1 column in train_date.csv)

Strategy
- analyze each feature set separately
- numerical patterns (i.e. direct pattern between numerical features)
        - is there an identifiable correlation between a numeric feature and classification tag
- unsure of categorical significance, need more research
- combine feature sets and identify relationship between types

Numerical Training Set
- need to identify relationships between features
- we can discard n-1 features if n features are numerically related (direct mapping)
- one of the easiest ways to view the relationship between features is to generate plots
        - brute force approach; last resort as sifting through plots will take long
        - problem: 970 x 970 grid of plots (will take a while to compute)
        - we can split it into chunks and view smaller sets at each time
        - can we split it intelligently or are we forced to do it iteratively for fixed chunk size
- can start by separating different types
        - only two distinct types: 'float64' and 'int64' where 'int64' has 2 counts
        - int64 counts: Id, Response (not useful)
- (Sep 18) might have to come back to numerical set => too large
- group features based on line and station
        - there seems to be only four lines (0, 1, 2, 3)
        - line 0: 24 stations, 168 features
        - line 1: 2 stations, 513 features
        - line 2: 3 stations, 42 features
        - line 3: 21 stations, 245 features
        - need to think about whether we run the prediction with the segment-specific analysis
        - alternative: combine all of the segments and perform analysis
- start with analysis of line 2 => 3 dataframes
        - smallest number of features
        - might give indication of how useful it is to segment the data based on line and station
        - dataframe identification df_0<station number>
- analysis of line 0 => 24 dataframes
        - look at all
        - dataframe identifcation df_0<station number>
- df_226
        - 15 features
        - nothing unusual in terms of value counts
        - NaN count conducted; want to check whether we can discard x parts from these features
        - we only discard a part from the station if total_measurements == 0
        - number of equal nan value counts = 13 => 13 features with the same number of nan counts
        - we want to determine which parts have len(df_226.columns) NaN's
        - use strictly_nan() function and pass transposed dataframe
                - takes a while to run as we're iterating over 1,183,747 columns and checkng each
        - we can then cut the dataframe to all non-nan parts and conduct feature analysis
        - 956,736 parts with strictly nan values
        - df_226_cut: newly created dataframe with significant parts
- df_226_cut
        - 15 features; 227,011 parts with data
        - run feature analysis (plots) and observe relations
        - generate 15 plots and try and keep scales consistent
                - 5 x 3 grid
                - use grid to compare data for each of the features
                - build generic plot building function
                - doesn't fare very well given the sheer size of the data
        - looking into more realistic feature engineering/selection algorithms

- Joan's suggestion
        - random sample the data set
        - ~10,000 records with all 970 features
        - worry about combining models once we've covered the entire sample set
        - POA
                - split dataset into segments (taking the first 300,000 records)
                - random sampling 10,000 records from the first 300,000

- 300,000 data set
        - split data set into failures and passes
        - want to see how features correspond to the part's response
                - plot feature value against response
                - might be able to determine thresholds for each feature (assuming binary tresholds)
        - 970 plots (will take time to run)
                - splitting plots into pieces
                - add params to function allowing specification of start and end indices for dataframe
        - decent output with each of the features plotted against the response
        - another added feature is switch frequency
                - number of times the response moves from 0 to 1 across all parts for a given feature
                - map feature values to 0 or 1 responses; remove all responses that have NaN values for the feature
                - check notebook for switch frequency plot
                - no frequency that exceeds 3000 => interest halt
        - conduct plot analysis of each feature against the switch frequency of said feature

Feature Analysis
- look at the plot for each feature and take note of interesting trends (e.g. number of outliers, ranges which map to responses)
        - few overall responses
                - L0_S20_F461
                - L1_S24_F746
                - L1_S24_751
                - L1_S24_F751
                - L0_S6_F118
                - L0_S8_F144
                - L0_S3_F88
                - L0_S7_F136
                - L0_S15_F409
                - L0_S2_F52
        - few positive responses (relative => criterion is usually 0.5)
                - L0_S8_F149
                - L1_S24_F756
                - L1_S24_F733
                - L1_S24_751
                - L0_S9_F205
                - L1_S24_F719
                - L0_S20_F463
                - L0_S20_F466
                - L0_S8_F146
                - L1_S24_F679
                - L0_S10_F269
                - 
        - few negative responses (relative => criterion is usually 0.5)
                - 
        - 1<=>1 mapping between response variables for the same feature value
                - few points is another criterion (less dense 
                - L0_S12_F338
                - L0_S23_F635
                - L0_S18_F435
                - L0_S14_F366
                - L0_S14_F358
                - L0_S12_F344
                - L0_S12_F340
                - L0_S15_F406
                - L0_S2_F32
                - L0_S23_F647
                - L0_S0_F14
                - L0_S19_F453
                - L0_S21_F492
                - L0_S0_F10
                - L0_S15_F400
                - L0_S12_F342
                - L0_S0_F6
                - L0_S0_F2
                - L0_S15_F412
                - L0_S14_F382
                - L0_S10_F239
                - L0_S2_F56
                - L0_S12_F334
                - L0_S9_F175
                - L0_S3_F76
                - L0_S15_F394
                - L0_S11_F298
                - L0_S22_F566
                - L0_S3_F68
                - L0_S0_F8
                - L0_S3_F92
                - L0_S21_F507
                - L0_S2_F40
                - L0_S14_F374
                - L0_S12_F336
- need to build a small model with analysis from plots
        - should be useful to get a prelim evaluation; gain experience so that model building is smoother
        - alternatively, build a model using all of the numerical features and check to see how that model fares on test numerical

- use SGDClassifier with all features
        - loss set to 'log'
        - leaving all of the other parameters the same
        - classifier does not like 'nan' values; need to research intelligent ways to fill nan values
                - SO recommendations: impute data with median() or 'ffill'
                - ffill: propagate last seen value to next valid

- http://www.jmlr.org/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf
        - less accurate methods: predictive value imputation, C4.5 distribution-based imputation
        - paper addresses imputation at prediction time; I'm going to try and apply the same methods for induction time
        - value imputation: estimate value in place of missing feature; more common in statistics community
        - distribution imputation: esimate conditional distribution of missing value; popular in non-Bayesian ML comm.
        - reduced feature: construct model but employ only features for a particular test case
        - multiple imputation: Monte Carlo approach that generates multiple simulated versions of data set; each are analyzed and results are combined to generate inference
        - predictive value imputation
                - common: replace missing value with feature's mean or mode (for real-valued, discrete-valued features, respectively)
                - average of values of other features for that particular record; useful for subjective features where there is a large variance between features for a record than records for a feature
                - predictive model that induces relationship between available values and missing values
        - distribution-based imputation
                - given distribution of values over a feature, we can estimate expected distribution of of record
                - weight assignments of missing values
                - generate a probability distribution of values and assign probabilistic weights
                - when missing value is encountered multiple pseudo-instances are generated with multiple values and weights corresponding to the estimated probability for that value
        - unique value imputation
                - replace each missing value with an arbitrary unique value
                - preferable in following conditions: missing value depends on value of class (target) and dependence is present in training and test data


- all features have NaN values for all parts
        - think I might try and implement a hybrid model; remove all features that have a nan value count past a certain threshold
                - need to view nan frequency across each feature and plot
                - once we remove some of the features, we use value imputation to replace the remaining null values
        - will eventually need to create a separate dataframe (df_numeric.copy()) => one dataframe instance is 2.2GB
        - 701 features with null value counts between 250000 and 300000 (max number of counts is 300000 as I'm only looking at 1/3 of the numeric data)
        - doesn't make much sense to use reduced-feature approach and only have ~190 features to work with
                - we want the model to reflect all possible instances regardless of how 'intelligent' it is
        - going to try a predictive value imputation with feature mean
                - Joan suggested that I use an interpolation for missing values; alternative is tried na where we leave NaNs as blanks => doesn't work when we get to classification stage
        - cannot use interpolation if lower_bound or upper_bound is NaN
        - cannot use ffill or bfill if upper or lower bound of any series is Nan 
        - trying three different imputation methods (need to delete copy of dataframe once model is built)
                - mean
                - median
                - average of features for a single part (most computationally expensive)
        - realized that I don't need to make a copy of the dataframe for each iteration
                - just need 'values' and we can store 'Response' as it is consistent across all transformations

- simple SGDClassifier model
        - numeric test data is used for submission; for now we want to determine model accuracy by splitting the data into 'training' and 'test' sets
                - 80% training and 20% testing (remember that this is already ~30% of the initial data set)
        - if the models with imputation are promising for the subsets, then I'll use the entire train_numeric data set and conduct an 80-20 split 
        - we conduct the same imputation on both the test and the training set for "consistency" (not sure how this is going to affect the model for feature-average imputation)
        - MCC evaluation for median and mean imputation: 1.0 (slightly odd)
                - 1 true positive, 2 true negatives, 0 false positives, 0 false negatives
        - can't apply a NaN transformation with a feature_average imputation due to a large number of rows with strictly NaN records across all features
        - need to investigate nature of MCC evaluation for median and mean imputations                

- feature correlation
        - one of the options is df.corr(); makes sense to plot the correlation for a small number of features
        - don't know how plotting 970 features is going to look like; can't reduce the number of features to use as we have to iterate over smaller sets
        - Eric's suggestion
                - build a graph using the correlation result between all 970 features
                - identify redundant nodes based on number of edges
        - will try PCA with all features (try and intelligently reduce the dimensions of the data)

- principal component analysis
        - all preprocessing techniques are extremely slow given the size of the data set (using all 1.8 million records with 970 features)

Categorical Training Set
- 2.5 GB worth of data
- unable to load this data into a dataframe

Supervised Learning Notes
- Generalized Linear Models
        - SGDClassifier
                - useful for dataset with large number of samples and features
        - loss="log" fits logistic regression model  

==========================

Numerical Classification
- confident that I can use regularization and a set of ensemble methods to obtain at least 70% overall accuracy with just the numerical data
- I'll have to think of a smart way to find the correlation between all of the numeric variables
- easiest way to conduct feature selection is to use SelectFromModel for a set of linear models and then take the top 'x' features based on their feature importance
- don't think I need to use a stratified sample of the data => ~1.1 million records and known 970 numerical features
- need to run nan value imputation prior to using models 
        - all features have missing values; finding all rows with missing values is too slow
        - we create a split as soon as we perform imputations => run correlations and model checks with each imputation type and then choose the better performing one
        - if both perform poorly, I'll have to think of a smarter way to impute missing values
        - performance bottleneck: can't load three instances of the same dataframe as I don't have enough memory for it
        - will have to run one imputation and all of the analyses sequentially
- might not be smart to plot correlation between all 970 features (970 x 970 plot)
- going to try grouping multiple features and generating a number of correlation plots
- alternative to plotting correlation/covariance between features
        - run features against estimators and look at features with close importance rankings
        - if non-linear intricacies are discarded (unlikely) then two redundant features should have the same level of importance
        - the only reason a correlation is considered is to reduce the feature set size
        - might not even have to do so if we can extract importance features from sets of ensemble methods
- ensemble method analysis
        - use a lesser number of estimators for determining important features; increase the number of estimators once we have a smaller feature set
        - output feature importances from merged SelectFromModel results to csv
        - ExtraTreesClassifier (n_estimators=10 on the entire training set i.e. all features)
                - accuracy : 99.4227384724 %
                - hit score (self-defined): 0.486381322957 %
                - positive-specific ("1")
                        - precision : 0.714
                        - recall: 0.005
                        - f1-score: 0.010
                        - support: 2056
                - high overall accuracy due to the proportion of positives to negatives in the data set => large number of false negatives
                - SelectFromModel returned 253 features => xt_sfm_features
        - RandomForestClassifier (n_estimators=10)
                - accuracy: 99.4278071102 %
                - hit score: 2.14007782101 %
                - positive specific
                        - precision: 0.688
                        - recall: 0.021
                        - f1-score: 0.042
                        - support: 2056
                - similar high overall accuracy as Extra Trees
                - performed better overall in terms of accuracy and hit score, in comparison to Extra Trees
                - lower scored features based on estimator ranking
                - SelectFromModel returned 258 features => rf_sfm_features
        - GradientBoostClassifier
                - accuracy: 99.4340021119 %
                - hit score: 5.30155642023
                - positive specific
                        - precision: 0.634
                        - recall: 0.053
                        - f1-score: 0.098
                        - support: 2056
                - highest overall accuracy and hit score across all ensemble methods
                - lower precision than random forests => tp / (tp + fp)
                - small number of overall features (23) from SelectFromModel => gb_sfm_features
                
Notes
- data represents part measurements
- large number of anonymized features
- can group features based on production and station lines
        - L<prod line>_S<station line>_F<feature id>
- three main feature types: numerical, categorical, date
        - date: timestamp for when measurement was taken
        - n features
        - i_th date column corresponds to i-1_th feature where iE[1, n+1]
        - assuming sequential relation between measurements and dates
        - (e.g. x measurements => x dates => 1 column in train_date.csv)

Strategy
- analyze each feature set separately
- numerical patterns (i.e. direct pattern between numerical features)
        - is there an identifiable correlation between a numeric feature and classification tag
- unsure of categorical significance, need more research
- combine feature sets and identify relationship between types

Numerical Training Set
- need to identify relationships between features
- we can discard n-1 features if n features are numerically related (direct mapping)
- one of the easiest ways to view the relationship between features is to generate plots
        - brute force approach; last resort as sifting through plots will take long
        - problem: 970 x 970 grid of plots (will take a while to compute)
        - we can split it into chunks and view smaller sets at each time
        - can we split it intelligently or are we forced to do it iteratively for fixed chunk size
- can start by separating different types
        - only two distinct types: 'float64' and 'int64' where 'int64' has 2 counts
        - int64 counts: Id, Response (not useful)
- (Sep 18) might have to come back to numerical set => too large
- group features based on line and station
        - there seems to be only four lines (0, 1, 2, 3)
        - line 0: 24 stations, 168 features
        - line 1: 2 stations, 513 features
        - line 2: 3 stations, 42 features
        - line 3: 21 stations, 245 features
        - need to think about whether we run the prediction with the segment-specific analysis
        - alternative: combine all of the segments and perform analysis
- start with analysis of line 2 => 3 dataframes
        - smallest number of features
        - might give indication of how useful it is to segment the data based on line and station
        - dataframe identification df_0<station number>
- analysis of line 0 => 24 dataframes
        - look at all
        - dataframe identifcation df_0<station number>
- df_226
        - 15 features
        - nothing unusual in terms of value counts
        - NaN count conducted; want to check whether we can discard x parts from these features
        - we only discard a part from the station if total_measurements == 0
        - number of equal nan value counts = 13 => 13 features with the same number of nan counts
        - we want to determine which parts have len(df_226.columns) NaN's
        - use strictly_nan() function and pass transposed dataframe
                - takes a while to run as we're iterating over 1,183,747 columns and checkng each
        - we can then cut the dataframe to all non-nan parts and conduct feature analysis
        - 956,736 parts with strictly nan values
        - df_226_cut: newly created dataframe with significant parts
- df_226_cut
        - 15 features; 227,011 parts with data
        - run feature analysis (plots) and observe relations
        - generate 15 plots and try and keep scales consistent
                - 5 x 3 grid
                - use grid to compare data for each of the features
                - build generic plot building function
                - doesn't fare very well given the sheer size of the data
        - looking into more realistic feature engineering/selection algorithms

- Joan's suggestion
        - random sample the data set
        - ~10,000 records with all 970 features
        - worry about combining models once we've covered the entire sample set
        - POA
                - split dataset into segments (taking the first 300,000 records)
                - random sampling 10,000 records from the first 300,000

- 300,000 data set
        - split data set into failures and passes
        - want to see how features correspond to the part's response
                - plot feature value against response
                - might be able to determine thresholds for each feature (assuming binary tresholds)
        - 970 plots (will take time to run)
                - splitting plots into pieces
                - add params to function allowing specification of start and end indices for dataframe
        - decent output with each of the features plotted against the response
        - another added feature is switch frequency
                - number of times the response moves from 0 to 1 across all parts for a given feature
                - map feature values to 0 or 1 responses; remove all responses that have NaN values for the feature
                - check notebook for switch frequency plot
                - no frequency that exceeds 3000 => interest halt
        - conduct plot analysis of each feature against the switch frequency of said feature

Feature Analysis
- look at the plot for each feature and take note of interesting trends (e.g. number of outliers, ranges which map to responses)
        - few overall responses
                - L0_S20_F461
                - L1_S24_F746
                - L1_S24_751
                - L1_S24_F751
                - L0_S6_F118
                - L0_S8_F144
                - L0_S3_F88
                - L0_S7_F136
                - L0_S15_F409
                - L0_S2_F52
        - few positive responses (relative => criterion is usually 0.5)
                - L0_S8_F149
                - L1_S24_F756
                - L1_S24_F733
                - L1_S24_751
                - L0_S9_F205
                - L1_S24_F719
                - L0_S20_F463
                - L0_S20_F466
                - L0_S8_F146
                - L1_S24_F679
                - L0_S10_F269
                - 
        - few negative responses (relative => criterion is usually 0.5)
                - 
        - 1<=>1 mapping between response variables for the same feature value
                - few points is another criterion (less dense 
                - L0_S12_F338
                - L0_S23_F635
                - L0_S18_F435
                - L0_S14_F366
                - L0_S14_F358
                - L0_S12_F344
                - L0_S12_F340
                - L0_S15_F406
                - L0_S2_F32
                - L0_S23_F647
                - L0_S0_F14
                - L0_S19_F453
                - L0_S21_F492
                - L0_S0_F10
                - L0_S15_F400
                - L0_S12_F342
                - L0_S0_F6
                - L0_S0_F2
                - L0_S15_F412
                - L0_S14_F382
                - L0_S10_F239
                - L0_S2_F56
                - L0_S12_F334
                - L0_S9_F175
                - L0_S3_F76
                - L0_S15_F394
                - L0_S11_F298
                - L0_S22_F566
                - L0_S3_F68
                - L0_S0_F8
                - L0_S3_F92
                - L0_S21_F507
                - L0_S2_F40
                - L0_S14_F374
                - L0_S12_F336
- need to build a small model with analysis from plots
        - should be useful to get a prelim evaluation; gain experience so that model building is smoother
        - alternatively, build a model using all of the numerical features and check to see how that model fares on test numerical

- use SGDClassifier with all features
        - loss set to 'log'
        - leaving all of the other parameters the same
        - classifier does not like 'nan' values; need to research intelligent ways to fill nan values
                - SO recommendations: impute data with median() or 'ffill'
                - ffill: propagate last seen value to next valid
        - http://www.jmlr.org/papers/volume8/saar-tsechansky07a/saar-tsechansky07a.pdf
        - less accurate methods: predictive value imputation, C4.5 distribution-based imputation
        - paper addresses imputation at prediction time; I'm going to try and apply the same methods for induction time
        - value imputation: estimate value in place of missing feature; more common in statistics community
        - distribution imputation: esimate conditional distribution of missing value; popular in non-Bayesian ML comm.
        - reduced feature: construct model but employ only features for a particular test case
        - all features have NaN values for all parts
        - think I might try and implement a hybrid model; remove all features that have a nan value count past a certain threshold
                - need to view nan frequency across each feature and plot
                - once we remove some of the features, we use value imputation to replace the remaining null values
        - will eventually need to create a separate dataframe (df_numeric.copy()) => one dataframe instance is 2.2GB
        - 701 features with null value counts between 250000 and 300000 (max number of counts is 300000 as I'm only looking at 1/3 of the numeric data)
        

Categorical Training Set
- 2.5 GB worth of data
- unable to load this data into a dataframe

Supervised Learning Notes
- Generalized Linear Models
        - SGDClassifier
                - useful for dataset with large number of samples and features
        - loss="log" fits logistic regression model
        -  
